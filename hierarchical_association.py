# -*- coding: utf-8 -*-
"""hierarchical_association.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FUFUejGaSDidUeLb_RxfH9PGHgyrb0rT
"""

from PIL import Image
import json


def crop_and_save_subobjects(json_data, image_path, output_dir='cropped_images'):

    import os
    os.makedirs(output_dir, exist_ok=True)

    for frame_data in json_data:
        img = Image.open(image_path)
        for detection in frame_data['detections']:
            if 'subobject' in detection:
                subobject = detection['subobject']
                bbox = subobject['bbox']


                cropped_img = img.crop(bbox)


                filename = f"{frame_data.get('frame_id', 0)}_{detection['id']}_{subobject['id']}_{subobject['object']}.jpg"
                output_path = os.path.join(output_dir, filename)

                cropped_img.save(output_path)


import cv2
import os

video_path = '/content/Traffic video  for object detection and tracking.mp4'
output_dir = "cropped_images"
os.makedirs(output_dir, exist_ok=True)
vidcap = cv2.VideoCapture(video_path)
success,image = vidcap.read()
count = 0
success = True
while success:
  success,image = vidcap.read()
  if not success:
    break
  cv2.imwrite(f"/content/frame{count}.jpg", image)     # save frame as JPEG file
  crop_and_save_subobjects(json_data, f"/content/frame{count}.jpg", output_dir)
  count += 1

import time
model = YOLO('yolov8n.pt')
video_path = '/content/Traffic video  for object detection and tracking.mp4'
cap = cv2.VideoCapture(video_path)

fps = cap.get(cv2.CAP_PROP_FPS)
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))


fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter('output.mp4', fourcc, fps, (frame_width, frame_height))

start_time = time.time()
frame_count = 0

while cap.isOpened():
    success, frame = cap.read()

    if success:

        results = model(frame)


        annotated_frame = results[0].plot()


        out.write(annotated_frame)

        frame_count += 1

        if frame_count % 10 == 0:
          end_time = time.time()
          elapsed_time = end_time - start_time
          fps = frame_count / elapsed_time
          print(f"Current FPS: {fps:.2f}")
          start_time = end_time
          frame_count = 0


    else:
        break


cap.release()
out.release()
print("Video processing complete.")

import json
import os
from PIL import Image

def process_detections(results, model):


    json_data = []
    for result in results:
        boxes = result.boxes
        frame_data = {}
        detections = []

        for i, box in enumerate(boxes):
            detection = {
                "object": model.names[int(box.cls[0])],
                "id": i,
                "bbox": box.xyxy[0].tolist(),
            }


            if model.names[int(box.cls[0])] == "car" and i % 2 == 0 and len(boxes) > i+1:
                  "object": "license_plate",
                  "id": i+1,
                  "bbox": boxes[i+1].xyxy[0].tolist()
              }
            detections.append(detection)

        frame_data["detections"] = detections
        json_data.append(frame_data)

    return json_data